{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d959b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xamon/Documenti/ricerca_formazione/custom_neural_networks/.venv/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fca3e3",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "Define a simple Feed Forward architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c8b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Apply first layer + activation\n",
    "        x = self.fc2(x)               # Apply second layer\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8306c303",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Testing FeedForwardNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e7191e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "# Create network\n",
    "model = FeedForwardNet(input_size=784, hidden_size=128, output_size=10)\n",
    "\n",
    "# Test with dummy data\n",
    "x = torch.randn(32, 784)  # 32 images, 784 pixels each\n",
    "output = model(x)\n",
    "\n",
    "print(output.shape)  # torch.Size([32, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7b44f",
   "metadata": {},
   "source": [
    "### Regression Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce814b8",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde68ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(10, 20),  # Input: 10 features → 20 hidden\n",
    "            nn.ReLU(),           # Non-linearity\n",
    "            nn.Linear(20, 1)     # Hidden: 20 → 1 output\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)  # Sequential applies layers in order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb59f3",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "395ffdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9490b",
   "metadata": {},
   "source": [
    "#### Define training subroutine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2693d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        # STEP 1: Forward pass - make predictions\n",
    "        predictions = model(X_train)\n",
    "        loss = criterion(predictions, y_train)\n",
    "        # STEP 2: Backward pass - calculate gradients\n",
    "        optimizer.zero_grad() # Clear old gradients (important!)\n",
    "        loss.backward() # Compute gradients via backpropagation\n",
    "        # STEP 3: Update weights\n",
    "        optimizer.step() # Apply the calculated gradients\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f399700",
   "metadata": {},
   "source": [
    "#### Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a13c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1158\n",
      "Epoch 20, Loss: 1.0120\n",
      "Epoch 40, Loss: 0.9589\n",
      "Epoch 60, Loss: 0.9203\n",
      "Epoch 80, Loss: 0.8846\n"
     ]
    }
   ],
   "source": [
    "# Create some random data\n",
    "X = torch.randn(100, 10)  # 100 samples, 10 features\n",
    "y = torch.randn(100, 1)   # 100 target values\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, X, y, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ce7a3",
   "metadata": {},
   "source": [
    "### Deeper Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad7ce4",
   "metadata": {},
   "source": [
    "#### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc22d120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define all layers\n",
    "        self.layer1 = nn.Linear(784, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.2)  # Drop 20% of neurons\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1: 784 → 512\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 2: 512 → 256\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 3: 256 → 128\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 4: 128 → 10 (no activation/dropout on output)\n",
    "        x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dda4ed",
   "metadata": {},
   "source": [
    "#### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14580dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = DeepNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609cd79",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d75e1",
   "metadata": {},
   "source": [
    "#### Replicate Satlin function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48150e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatLin(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.clamp(x, min=0, max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39085ba1",
   "metadata": {},
   "source": [
    "#### Create perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5823a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1. Definisci il layer lineare\n",
    "        self.layer = nn.Linear(3, 1) \n",
    "        \n",
    "        # 2. Definisci l'attivazione (Crei l'istanza qui!)\n",
    "        self.activation = SatLin() \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Passaggio nel layer lineare (usa 'self.layer', non 'layer1')\n",
    "        x = self.layer(x)\n",
    "        \n",
    "        # Passaggio nell'attivazione\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5fd8119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizio addestramento...\n",
      "Epoca [20/100], Errore (Loss): 0.0000\n",
      "Epoca [40/100], Errore (Loss): 0.0000\n",
      "Epoca [60/100], Errore (Loss): 0.0000\n",
      "Epoca [80/100], Errore (Loss): 0.0000\n",
      "Epoca [100/100], Errore (Loss): 0.0000\n",
      "\n",
      "--- Test Finale ---\n",
      "Pattern A  (Target 1) -> Predizione: 1.0000\n",
      "Pattern B  (Target 0) -> Predizione: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Prepariamo i Dati (I due pattern) ---\n",
    "# Immaginiamo che l'input abbia 3 caratteristiche.\n",
    "# Pattern 1: [1, 1, 1] -> Deve dare output 0\n",
    "# Pattern 2: [0, 0, 0] -> Deve dare output 1\n",
    "\n",
    "X_train = torch.tensor([\n",
    "    [1.0, -1.0, 1.0],  # Pattern A\n",
    "    [-1.0, -1.0, 1.0]   # Pattern B\n",
    "])\n",
    "\n",
    "# I target corrispondenti (etichette)\n",
    "y_train = torch.tensor([\n",
    "    [1.0],  # Target per Pattern A\n",
    "    [0.0]   # Target per Pattern B\n",
    "])\n",
    "\n",
    "# --- 3. Setup del Training ---\n",
    "model = Perceptron()\n",
    "\n",
    "# Funzione di errore: calcola la distanza tra previsione e realtà\n",
    "criterion = nn.MSELoss() \n",
    "\n",
    "# Ottimizzatore: Modifica i pesi. Il learning rate (lr) decide quanto \"grandi\" sono i passi\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# --- 4. Il Ciclo di Addestramento (Loop) ---\n",
    "epochs = 100 # Quante volte gli facciamo vedere i dati\n",
    "\n",
    "print(\"Inizio addestramento...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # A. Reset dei gradienti (fondamentale in PyTorch!)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # B. Forward pass: Il modello fa una previsione\n",
    "    outputs = model(X_train)\n",
    "    \n",
    "    # C. Calcolo dell'errore (Loss)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # D. Backward pass: Calcola di quanto correggere i pesi\n",
    "    loss.backward()\n",
    "    \n",
    "    # E. Step: Aggiorna effettivamente i pesi\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Stampiamo ogni 20 epoche come sta andando\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoca [{epoch+1}/{epochs}], Errore (Loss): {loss.item():.4f}')\n",
    "\n",
    "# --- 5. Verifica Finale ---\n",
    "print(\"\\n--- Test Finale ---\")\n",
    "with torch.no_grad(): # Disabilita il calcolo gradienti per il test\n",
    "    test_output = model(X_train)\n",
    "    print(f\"Pattern A  (Target 1) -> Predizione: {test_output[0].item():.4f}\")\n",
    "    print(f\"Pattern B  (Target 0) -> Predizione: {test_output[1].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06464f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
